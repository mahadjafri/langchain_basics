{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AsMFKCALr3y"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aurelio-labs/langchain-course/blob/main/chapters/07-lcel.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PQji83qLpVC"
      },
      "source": [
        "#### LangChain Essentials Course"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22dLBWVZLpVD"
      },
      "source": [
        "# LangChains Expression Language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiRUqmfBLpVE"
      },
      "source": [
        "LangChain is one of the most popular open source libraries for AI Engineers. It's goal is to abstract away the complexity in building AI software, provide easy-to-use building blocks, and make it easier when switching between AI service providers.\n",
        "\n",
        "In this example, we will introduce LangChain's Expression Langauge (LCEL), abstracting a full chain and understanding how it will work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i690tIabLwb_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68c61aae-f814-4a50-88e8-4c2ca99d737b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: langchain 0.3.27\n",
            "Uninstalling langchain-0.3.27:\n",
            "  Successfully uninstalled langchain-0.3.27\n",
            "Found existing installation: google-generativeai 0.8.5\n",
            "Uninstalling google-generativeai-0.8.5:\n",
            "  Successfully uninstalled google-generativeai-0.8.5\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.2/270.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.0/444.0 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.8/241.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.8/378.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y langchain google-generativeai\n",
        "!pip install -qU \\\n",
        "  \"pydantic>=2.11.4,<3\" \\\n",
        "  \"langchain-core>=0.3.70,<0.4\" \\\n",
        "  \"langchain-community>=0.3.27,<0.4\" \\\n",
        "  \"langchain-google-genai>=2.1.10\" \\\n",
        "  \"google-genai>=0.7.0\" \\\n",
        "  \"langsmith>=0.3.4\" \\\n",
        "  \"docarray==0.40.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfpD-R6sLpVE"
      },
      "source": [
        "---\n",
        "\n",
        "> ⚠️ If using LangSmith, add your API key below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "exBqQHgqLpVE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from google.colab import userdata\n",
        "\n",
        "# must enter API key\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get(\"LANGCHAIN_API_KEY\") or \\\n",
        "    getpass(\"Enter LangSmith API Key: \")\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"aurelioai-langchain-course-lcel-openai\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYGeByKjLpVF"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQbgErgdLpVF"
      },
      "source": [
        "## Traditional Chains vs LCEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dyluns_ELpVF"
      },
      "source": [
        "In this section we're going to dive into a basic example using the traditional method for building chains before jumping into LCEL. We will build a pipeline where the user must input a specific topic, and then the LLM will look and return a report on the specified topic. Generating a _research report_ for the user."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTZTTiK7LpVF"
      },
      "source": [
        "### Traditional LLMChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDHfbKZ7LpVF"
      },
      "source": [
        "The `LLMChain` is the simplest chain originally introduced in LangChain. This chain takes a prompt, feeds it into an LLM, and _optionally_ adds an output parsing step before returning the result.\n",
        "\n",
        "Let's see how we construct this using the traditional method, for this we need:\n",
        "\n",
        "* `prompt` — a `PromptTemplate` that will be used to generate the prompt for the LLM.\n",
        "* `llm` — the LLM we will be using to generate the output.\n",
        "* `output_parser` — an optional output parser that will be used to parse the structured output of the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Jyz2vWLoLpVF"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "prompt_template = \"Give me a small report on {topic}\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=prompt_template\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rykro39YLpVF"
      },
      "source": [
        "For the LLM, we'll start by initializing our connection to the OpenAI API. We do need an OpenAI API key, which you can get from the [OpenAI platform](https://platform.openai.com/api-keys).\n",
        "\n",
        "We will use the `gpt-4o-mini` model with a `temperature` of `0.0`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Uvs7hILRLpVF"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\") or getpass(\n",
        "    \"Enter Gemini API Key: \"\n",
        ")\n",
        "\n",
        "openai_model = \"gemini-1.5-flash\"\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=openai_model,\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    # other params...\n",
        ")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhs6_3c4LpVF",
        "outputId": "5eba8836-e0e2-4943-9612-e4d46c9937f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello there! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--cff589e1-7a11-4539-9114-66742b2bc650-0', usage_metadata={'input_tokens': 2, 'output_tokens': 11, 'total_tokens': 13, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "llm_out = llm.invoke(\"Hello there\")\n",
        "llm_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V39qjMpLLpVG"
      },
      "source": [
        "Then we define our output parser, this will be used to parse the output of the LLM. In this case, we will use the `StrOutputParser` which will parse the `AIMessage` output from our LLM into a single string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "46qTjklnLpVG"
      },
      "outputs": [],
      "source": [
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "z0uwxedBLpVG",
        "outputId": "62a60ebf-0e22-4439-dd64-e43dfc333570"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello there! How can I help you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "out = output_parser.invoke(llm_out)\n",
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk4CUy52LpVG"
      },
      "source": [
        "Through the `LLMChain` class we can place each of our components into a linear `chain`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PrFk1J9dLpVG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "032064bd-4d6c-4584-be38-ed401dd224bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2960353250.py:3: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(prompt=prompt, llm=llm, output_parser=output_parser)\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(prompt=prompt, llm=llm, output_parser=output_parser)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjNsVMVpLpVG"
      },
      "source": [
        "Note that the `LLMChain` _was_ deprecated in LangChain `0.1.17`, the expected way of constructing these chains today is through LCEL, which we'll cover in a moment.\n",
        "\n",
        "We can `invoke` our `chain`, providing a `topic` that we'd like to be researched."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDDq8Cq7LpVG",
        "outputId": "62ab3720-9b07-4f2b-976c-23ff12786f39"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'topic': 'retrieval augmented generation',\n",
              " 'text': \"## Retrieval Augmented Generation (RAG): A Small Report\\n\\nRetrieval Augmented Generation (RAG) is a powerful technique that enhances large language models (LLMs) by combining their generative capabilities with external knowledge retrieval.  Instead of relying solely on the knowledge embedded within the model's parameters, RAG systems access and incorporate information from external knowledge bases, databases, or documents relevant to the user's prompt. This significantly improves the accuracy, factual consistency, and overall quality of the generated text.\\n\\n**How it works:**\\n\\nRAG typically involves three main steps:\\n\\n1. **Retrieval:**  Given a user prompt, a retrieval module identifies relevant information from an external knowledge source. This often involves techniques like keyword matching, semantic search (using embeddings), or vector databases.  The quality of the retrieval is crucial for the success of the entire system.\\n\\n2. **Augmentation:** The retrieved information is then integrated with the user's prompt. This can be done by simply concatenating the retrieved text with the prompt, or through more sophisticated methods that highlight the relevance of specific passages.\\n\\n3. **Generation:** The augmented prompt is fed into the LLM, which generates the final output. The LLM now has access to both its internal knowledge and the relevant external information, leading to a more informed and accurate response.\\n\\n**Advantages of RAG:**\\n\\n* **Improved Accuracy and Factuality:**  By grounding the generation process in external data, RAG reduces hallucinations and ensures the output aligns with known facts.\\n* **Access to Up-to-Date Information:** LLMs are trained on static datasets. RAG allows access to the latest information, making the generated content more current and relevant.\\n* **Handling of Specialized Knowledge:** RAG can be used to incorporate domain-specific knowledge that may not be present in the LLM's training data.\\n* **Explainability and Transparency:**  The retrieved sources can provide context and justification for the generated output, increasing transparency and trust.\\n\\n**Challenges of RAG:**\\n\\n* **Retrieval Effectiveness:** The accuracy of the retrieved information is paramount.  Poor retrieval can lead to inaccurate or irrelevant outputs.\\n* **Computational Cost:**  The retrieval and augmentation steps add computational overhead compared to using LLMs alone.\\n* **Data Bias and Quality:**  The quality and potential biases in the external knowledge source directly impact the quality of the generated text.\\n* **Hallucination Mitigation:** While RAG reduces hallucinations, it doesn't eliminate them entirely.  The LLM can still generate incorrect information based on misinterpretations of the retrieved data.\\n\\n\\n**Conclusion:**\\n\\nRAG represents a significant advancement in LLM applications. By bridging the gap between the generative capabilities of LLMs and the vastness of external knowledge, RAG systems offer a more robust, accurate, and reliable approach to natural language processing tasks.  Ongoing research focuses on improving retrieval techniques, managing computational costs, and mitigating the remaining challenges to unlock the full potential of this powerful technology.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "result = chain.invoke(\"retrieval augmented generation\")\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbHqc1qLLpVG"
      },
      "source": [
        "We can view a formatted version of this output using the `Markdown` display:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "id": "hBxPHOZ-LpVG",
        "outputId": "c5e948c6-f74c-4e77-a292-a08a8fbd28c4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Retrieval Augmented Generation (RAG): A Small Report\n\nRetrieval Augmented Generation (RAG) is a powerful technique that enhances large language models (LLMs) by combining their generative capabilities with external knowledge retrieval.  Instead of relying solely on the knowledge embedded within the model's parameters, RAG systems access and incorporate information from external knowledge bases, databases, or documents relevant to the user's prompt. This significantly improves the accuracy, factual consistency, and overall quality of the generated text.\n\n**How it works:**\n\nRAG typically involves three main steps:\n\n1. **Retrieval:**  Given a user prompt, a retrieval module identifies relevant information from an external knowledge source. This often involves techniques like keyword matching, semantic search (using embeddings), or vector databases.  The quality of the retrieval is crucial for the success of the entire system.\n\n2. **Augmentation:** The retrieved information is then integrated with the user's prompt. This can be done by simply concatenating the retrieved text with the prompt, or through more sophisticated methods that highlight the relevance of specific passages.\n\n3. **Generation:** The augmented prompt is fed into the LLM, which generates the final output. The LLM now has access to both its internal knowledge and the relevant external information, leading to a more informed and accurate response.\n\n**Advantages of RAG:**\n\n* **Improved Accuracy and Factuality:**  By grounding the generation process in external data, RAG reduces hallucinations and ensures the output aligns with known facts.\n* **Access to Up-to-Date Information:** LLMs are trained on static datasets. RAG allows access to the latest information, making the generated content more current and relevant.\n* **Handling of Specialized Knowledge:** RAG can be used to incorporate domain-specific knowledge that may not be present in the LLM's training data.\n* **Explainability and Transparency:**  The retrieved sources can provide context and justification for the generated output, increasing transparency and trust.\n\n**Challenges of RAG:**\n\n* **Retrieval Effectiveness:** The accuracy of the retrieved information is paramount.  Poor retrieval can lead to inaccurate or irrelevant outputs.\n* **Computational Cost:**  The retrieval and augmentation steps add computational overhead compared to using LLMs alone.\n* **Data Bias and Quality:**  The quality and potential biases in the external knowledge source directly impact the quality of the generated text.\n* **Hallucination Mitigation:** While RAG reduces hallucinations, it doesn't eliminate them entirely.  The LLM can still generate incorrect information based on misinterpretations of the retrieved data.\n\n\n**Conclusion:**\n\nRAG represents a significant advancement in LLM applications. By bridging the gap between the generative capabilities of LLMs and the vastness of external knowledge, RAG systems offer a more robust, accurate, and reliable approach to natural language processing tasks.  Ongoing research focuses on improving retrieval techniques, managing computational costs, and mitigating the remaining challenges to unlock the full potential of this powerful technology."
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "display(Markdown(result[\"text\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cbmq0DfeLpVG"
      },
      "source": [
        "That is a simple `LLMChain` using the traditional LangChain method. Now let's move onto LCEL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWets8OpLpVG"
      },
      "source": [
        "## LangChain Expression Language (LCEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Lr5NA0xLpVG"
      },
      "source": [
        "**L**ang**C**hain **E**xpression **L**anguage (LCEL) is the recommended approach to building chains in LangChain. Having superceeded the traditional methods with `LLMChain`, etc. LCEL gives us a more flexible system for building chains. The pipe operator `|` is used by LCEL to _chain_ together components. Let's see how we'd construct an `LLMChain` using LCEL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8R-9b7ulLpVH"
      },
      "outputs": [],
      "source": [
        "lcel_chain = prompt | llm | output_parser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdJvzetzLpVH"
      },
      "source": [
        "We can `invoke` this chain in the same way as we did before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "XKWzIwpCLpVH",
        "outputId": "d8e3319f-3131-4466-b414-4a5d7b1b2b07"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"## Retrieval Augmented Generation (RAG): A Small Report\\n\\nRetrieval Augmented Generation (RAG) is a paradigm shift in large language model (LLM) applications, addressing some key limitations of LLMs operating solely on their internal knowledge.  Instead of relying solely on pre-trained knowledge, RAG systems augment LLMs with external knowledge sources, typically a vector database containing relevant documents.  This allows the model to access and process information beyond its training data, leading to several advantages:\\n\\n**How it works:**\\n\\n1. **Retrieval:**  Given a user prompt, a retrieval module searches the external knowledge base for relevant documents. This often involves embedding both the prompt and the documents into a vector space and using similarity search techniques (e.g., cosine similarity) to identify the most pertinent documents.\\n\\n2. **Augmentation:** The retrieved documents are then provided as context to the LLM alongside the original prompt.  This allows the LLM to generate a response informed by the specific information found in the retrieved documents.\\n\\n3. **Generation:** The LLM generates a response based on both its internal knowledge and the newly retrieved information.  This results in more accurate, up-to-date, and contextually relevant outputs.\\n\\n\\n**Advantages of RAG:**\\n\\n* **Improved Accuracy and Factuality:** Access to external data sources reduces hallucinations and improves the accuracy of the generated text, especially for factual queries.\\n* **Up-to-date Information:** LLMs are trained on static datasets. RAG allows access to the latest information, overcoming the limitations of outdated training data.\\n* **Handling Specialized Knowledge:** RAG enables LLMs to handle niche topics or domains by accessing relevant specialized documents.\\n* **Explainability and Traceability:**  The retrieved documents provide a degree of explainability, allowing users to understand the basis of the LLM's response.\\n\\n\\n**Challenges of RAG:**\\n\\n* **Retrieval Effectiveness:** The quality of the retrieved documents significantly impacts the quality of the generated response.  Poor retrieval can lead to inaccurate or irrelevant outputs.\\n* **Computational Cost:**  The retrieval and augmentation steps add computational overhead compared to using an LLM alone.\\n* **Data Management:**  Maintaining and updating the external knowledge base requires significant effort and resources.\\n* **Hallucination Mitigation:** While RAG reduces hallucinations, it doesn't eliminate them entirely.  The LLM can still misinterpret or misrepresent the retrieved information.\\n\\n\\n**Conclusion:**\\n\\nRAG represents a significant advancement in LLM applications, offering a pathway to more accurate, up-to-date, and contextually relevant responses.  While challenges remain, ongoing research and development are addressing these limitations, paving the way for wider adoption of RAG in various applications, including question answering, chatbots, and content generation.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "result = lcel_chain.invoke(\"retrieval augmented generation\")\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7E8_msHLpVH"
      },
      "source": [
        "The output format is slightly different, but the underlying functionality and content being output is the same. As before, we can view a formatted version of this output using the `Markdown` display:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "id": "42uPzB1JLpVH",
        "outputId": "3306b012-292a-48a3-a4e9-2c83a5ce9f04"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Retrieval Augmented Generation (RAG): A Small Report\n\nRetrieval Augmented Generation (RAG) is a paradigm shift in large language model (LLM) applications, addressing some key limitations of LLMs operating solely on their internal knowledge.  Instead of relying solely on pre-trained knowledge, RAG systems augment LLMs with external knowledge sources, typically a vector database containing relevant documents.  This allows the model to access and process information beyond its training data, leading to several advantages:\n\n**How it works:**\n\n1. **Retrieval:**  Given a user prompt, a retrieval module searches the external knowledge base for relevant documents. This often involves embedding both the prompt and the documents into a vector space and using similarity search techniques (e.g., cosine similarity) to identify the most pertinent documents.\n\n2. **Augmentation:** The retrieved documents are then provided as context to the LLM alongside the original prompt.  This allows the LLM to generate a response informed by the specific information found in the retrieved documents.\n\n3. **Generation:** The LLM generates a response based on both its internal knowledge and the newly retrieved information.  This results in more accurate, up-to-date, and contextually relevant outputs.\n\n\n**Advantages of RAG:**\n\n* **Improved Accuracy and Factuality:** Access to external data sources reduces hallucinations and improves the accuracy of the generated text, especially for factual queries.\n* **Up-to-date Information:** LLMs are trained on static datasets. RAG allows access to the latest information, overcoming the limitations of outdated training data.\n* **Handling Specialized Knowledge:** RAG enables LLMs to handle niche topics or domains by accessing relevant specialized documents.\n* **Explainability and Traceability:**  The retrieved documents provide a degree of explainability, allowing users to understand the basis of the LLM's response.\n\n\n**Challenges of RAG:**\n\n* **Retrieval Effectiveness:** The quality of the retrieved documents significantly impacts the quality of the generated response.  Poor retrieval can lead to inaccurate or irrelevant outputs.\n* **Computational Cost:**  The retrieval and augmentation steps add computational overhead compared to using an LLM alone.\n* **Data Management:**  Maintaining and updating the external knowledge base requires significant effort and resources.\n* **Hallucination Mitigation:** While RAG reduces hallucinations, it doesn't eliminate them entirely.  The LLM can still misinterpret or misrepresent the retrieved information.\n\n\n**Conclusion:**\n\nRAG represents a significant advancement in LLM applications, offering a pathway to more accurate, up-to-date, and contextually relevant responses.  While challenges remain, ongoing research and development are addressing these limitations, paving the way for wider adoption of RAG in various applications, including question answering, chatbots, and content generation."
          },
          "metadata": {}
        }
      ],
      "source": [
        "display(Markdown(result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbutFcpyLpVI"
      },
      "source": [
        "### How Does the Pipe Operator Work?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIjOY7VILpVI"
      },
      "source": [
        "Before moving onto other LCEL features, let's take a moment to understand what the pipe operator `|` is doing and _how_ it works.\n",
        "\n",
        "Functionality wise, the pipe tells you that whatever the _left_ side outputs will be fed as input into the _right_ side. In the example of `prompt | llm | output_parser`, we see that `prompt` feeds into `llm` feeds into `output_parser`.\n",
        "\n",
        "The pipe operator is a way of chaining together components, and is a way of saying that whatever the _left_ side outputs will be fed as input into the _right_ side.\n",
        "\n",
        "Let's make a basic class named `Runnable` that will transform our a provided function into a _runnable_ class that we will then use with the pipe `|` operator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9xTvwvF4LpVI"
      },
      "outputs": [],
      "source": [
        "class Runnable:\n",
        "    def __init__(self, func):\n",
        "        self.func = func\n",
        "    def __or__(self, other):\n",
        "        def chained_func(*args, **kwargs):\n",
        "            return other.invoke(self.func(*args, **kwargs))\n",
        "        return Runnable(chained_func)\n",
        "    def invoke(self, *args, **kwargs):\n",
        "        return self.func(*args, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2Vg_PhXLpVI"
      },
      "source": [
        "With the `Runnable` class, we will be able wrap a function into the class, allowing us to then chain together multiple of these _runnable_ functions using the `__or__` method.\n",
        "\n",
        "First, let's create a few functions that we'll chain together:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "G7HCw9-VLpVI"
      },
      "outputs": [],
      "source": [
        "def add_five(x):\n",
        "    return x+5\n",
        "\n",
        "def sub_five(x):\n",
        "    return x-5\n",
        "\n",
        "def mul_five(x):\n",
        "    return x*5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0fDvmtWLpVI"
      },
      "source": [
        "Now we wrap our functions with the `Runnable`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "LfnBBcbxLpVI"
      },
      "outputs": [],
      "source": [
        "add_five_runnable = Runnable(add_five)\n",
        "sub_five_runnable = Runnable(sub_five)\n",
        "mul_five_runnable = Runnable(mul_five)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jHLbl_7LpVI"
      },
      "source": [
        "Finally, we can chain these together using the `__or__` method from the `Runnable` class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHFo61TwLpVI",
        "outputId": "139476a0-a2c2-49ba-a995-aa155c9cc1b8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "chain = (add_five_runnable).__or__(sub_five_runnable).__or__(mul_five_runnable)\n",
        "\n",
        "chain.invoke(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oim1zWHLpVI"
      },
      "source": [
        "So we can see that we're able to chain together our functions using `__or__`. The pipe `|` operator is simply a shortcut for the `__or__` method, so we can create the exact same chain like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uh3-jF8pLpVJ",
        "outputId": "9a0aab8b-43ac-47ea-c542-431aba25bbf6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "chain = add_five_runnable | sub_five_runnable | mul_five_runnable\n",
        "\n",
        "chain.invoke(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Lnj00T8LpVJ"
      },
      "source": [
        "## LCEL `RunnableLambda`\n",
        "\n",
        "The `RunnableLambda` class is LangChain's built-in method for constructing a _runnable_ object from a function. That is, it does the same thing as the custom `Runnable` class we created earlier. Let's try it out with the same functions as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "BmMH8GzVLpVJ"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "add_five_runnable = RunnableLambda(add_five)\n",
        "sub_five_runnable = RunnableLambda(sub_five)\n",
        "mul_five_runnable = RunnableLambda(mul_five)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkpwIKq6LpVJ"
      },
      "source": [
        "We chain these together again with the pipe `|` operator:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "9e58vaSELpVJ"
      },
      "outputs": [],
      "source": [
        "chain = add_five_runnable | sub_five_runnable | mul_five_runnable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FdqbdAyLpVJ"
      },
      "source": [
        "And call them using the `invoke` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqHBMT8ULpVJ",
        "outputId": "79ec44b0-81f5-4315-e073-d2fc0444a4d9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "chain.invoke(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47mu4xxqLpVJ"
      },
      "source": [
        "Now we want to try something a little more testing, so this time we will generate a report, and we will try and edit that report using this functionallity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "xRJLzshqLpVJ"
      },
      "outputs": [],
      "source": [
        "prompt_str = \"give me a small report about {topic}\"\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=prompt_str\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "lfl_s4VALpVJ"
      },
      "outputs": [],
      "source": [
        "chain = prompt | llm | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "WSMlRM8wLpVJ",
        "outputId": "f995782d-034c-4d72-86d6-d0e3537c28cd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## A Brief Report on Artificial Intelligence\n\nArtificial intelligence (AI) is rapidly transforming various sectors, impacting everything from healthcare and finance to transportation and entertainment.  While the term encompasses a broad range of technologies, current advancements center around machine learning (ML) and deep learning (DL).  ML algorithms allow computers to learn from data without explicit programming, while DL utilizes artificial neural networks with multiple layers to analyze complex data patterns.\n\n**Key Developments:**\n\n* **Generative AI:**  Models like GPT-3 and DALL-E 2 demonstrate impressive capabilities in generating human-quality text, images, and other media, opening new avenues for creativity and content creation.  However, concerns regarding misinformation and ethical implications are prominent.\n* **Improved Natural Language Processing (NLP):**  AI's ability to understand and process human language continues to improve, leading to more sophisticated chatbots, language translation tools, and sentiment analysis applications.\n* **Computer Vision Advancements:**  AI-powered image recognition and object detection are becoming increasingly accurate and efficient, with applications in autonomous vehicles, medical diagnosis, and security systems.\n\n**Challenges and Concerns:**\n\n* **Bias and Fairness:** AI systems trained on biased data can perpetuate and amplify existing societal inequalities.  Addressing bias in algorithms and datasets is crucial for responsible AI development.\n* **Job Displacement:** Automation driven by AI raises concerns about potential job losses across various industries.  Reskilling and upskilling initiatives are necessary to mitigate this impact.\n* **Ethical Considerations:**  The use of AI in surveillance, autonomous weapons, and decision-making processes raises significant ethical questions that require careful consideration and regulation.\n\n**Conclusion:**\n\nAI is a powerful technology with the potential to solve complex problems and improve lives.  However, its development and deployment must be guided by ethical principles and a focus on mitigating potential risks.  Ongoing research, responsible development, and robust regulatory frameworks are essential to harness the benefits of AI while addressing its challenges."
          },
          "metadata": {}
        }
      ],
      "source": [
        "result = chain.invoke(\"AI\")\n",
        "display(Markdown(result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE7HOxwaLpVJ"
      },
      "source": [
        "Here we are making two functions, `extract_fact` to pull out the main content of our text and `replace_word` that will replace AI with Skynet!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Wc7ws8FkLpVJ"
      },
      "outputs": [],
      "source": [
        "def extract_fact(x):\n",
        "    if \"\\n\\n\" in x:\n",
        "        return \"\\n\".join(x.split(\"\\n\\n\")[1:])\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "old_word = \"AI\"\n",
        "new_word = \"skynet\"\n",
        "\n",
        "def replace_word(x):\n",
        "    return x.replace(old_word, new_word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktHrg5iSLpVK"
      },
      "source": [
        "Lets wrap these functions and see what the output is!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "_vFeqMY-LpVK"
      },
      "outputs": [],
      "source": [
        "extract_fact_runnable = RunnableLambda(extract_fact)\n",
        "replace_word_runnable = RunnableLambda(replace_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "TDWN-tQzLpVK"
      },
      "outputs": [],
      "source": [
        "chain = prompt | llm | output_parser | extract_fact_runnable | replace_word_runnable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "OCODP8vLLpVK",
        "outputId": "2ef1ad84-6f75-407b-b776-11d2fdc46f15"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Retrieval Augmented Generation (RAG) is a paradigm shift in large language model (LLM) applications, addressing limitations of traditional LLMs by augmenting their capabilities with external knowledge sources.  Instead of relying solely on the knowledge embedded during training, RAG systems retrieve relevant information from a knowledge base before generating a response. This allows for:\n* **Access to up-to-date information:** LLMs are trained on static datasets, making them unaware of recent events or updates. RAG overcomes this by connecting to dynamic knowledge bases like databases, web pages, or internal documents.\n* **Improved accuracy and factual consistency:** By grounding responses in retrieved evidence, RAG reduces hallucinations (fabricating information) and improves the accuracy and reliability of the generated text.\n* **Handling complex or specialized queries:**  LLMs may struggle with niche topics or require extensive context. RAG allows them to focus on reasoning and generation while leveraging external sources for the necessary factual information.\n* **Enhanced explainability:**  The retrieved sources provide context and justification for the generated response, increasing transparency and trust.\n\n**How it works:**  A RAG system typically involves three main components:\n1. **Retrieval:** A retrieval module identifies relevant information from the knowledge base based on the user's query.  This often involves techniques like keyword matching, semantic search, or vector databases.\n2. **Contextualization:** The retrieved information is then processed and contextualized to be suitable for the LLM. This might involve formatting, summarization, or highlighting key aspects.\n3. **Generation:** The LLM receives both the user's query and the retrieved context as input and generates a response.\n\n**Challenges:**  Despite its advantages, RAG faces challenges including:\n* **Retrieval effectiveness:**  The quality of the generated response heavily depends on the relevance and accuracy of the retrieved information.  Poor retrieval can lead to inaccurate or irrelevant outputs.\n* **Computational cost:**  The retrieval and contextualization steps add computational overhead, potentially impacting performance and scalability.\n* **Knowledge base management:**  Maintaining and updating a large and accurate knowledge base can be challenging and resource-intensive.\n\n**Conclusion:** RAG represents a significant advancement in LLM applications, offering improved accuracy, up-to-dateness, and explainability.  While challenges remain, ongoing research and development are addressing these issues, paving the way for more robust and reliable skynet systems.  The future of RAG likely involves more sophisticated retrieval methods, efficient knowledge base management techniques, and seamless integration with various LLM architectures."
          },
          "metadata": {}
        }
      ],
      "source": [
        "result = chain.invoke(\"retrieval augmented generation\")\n",
        "display(Markdown(result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHSumR1tLpVK"
      },
      "source": [
        "Those are our `RunnableLambda` functions. It's worth noting that all inputs to these functions are expected to be a SINGLE arguments. If you have a function that accepts multiple arguments, you can input a dictionary with keys, then unpack them inside the function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIXKLyQKLpVK"
      },
      "source": [
        "## LCEL `RunnableParallel` and `RunnablePassthrough`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avFhxg_pLpVK"
      },
      "source": [
        "LCEL provides us with various `Runnable` classes that allow us to control the flow of data and execution order through our chains. Two of these are `RunnableParallel` and `RunnablePassthrough`.\n",
        "\n",
        "* `RunnableParallel` — allows us to run multiple `Runnable` instances in parallel. Acting almost as a Y-fork in the chain.\n",
        "\n",
        "* `RunnablePassthrough` — allows us to pass through a variable to the next `Runnable` without modification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTsqIAufLpVK"
      },
      "source": [
        "To see these runnables in action, we will create two data sources, each source provides specific information but to answer the question we will need both to fed to the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "zaD678FMLpVK"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain.vectorstores import DocArrayInMemorySearch\n",
        "from google.colab import userdata\n",
        "import os\n",
        "from pydantic import ValidationError\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "vecstore_a = DocArrayInMemorySearch.from_texts(\n",
        "    [\n",
        "        \"half the info is here\",\n",
        "        \"DeepSeek-V3 was released in December 2024\"\n",
        "    ],\n",
        "    embedding=embedding\n",
        ")\n",
        "vecstore_b = DocArrayInMemorySearch.from_texts(\n",
        "    [\n",
        "        \"the other half of the info is here\",\n",
        "        \"the DeepSeek-V3 LLM is a mixture of experts model with 671B parameters\"\n",
        "    ],\n",
        "    embedding=embedding\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ub4pjRrBLpVK"
      },
      "source": [
        "Here you can see the prompt does have three inputs, two for context and one for the question itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "pnxXrw-CLpVK"
      },
      "outputs": [],
      "source": [
        "prompt_str = \"\"\"Using the context provided, answer the user's question.\n",
        "Context:\n",
        "{context_a}\n",
        "{context_b}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "jL3YN1c1LpVK"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(prompt_str),\n",
        "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YchbLoKLpVK"
      },
      "source": [
        "Here we are wrapping our vector stores as retrievers so they can be fitted into one big retrieval variable to be used by the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "xkhFV8-SLpVL"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "\n",
        "retriever_a = vecstore_a.as_retriever()\n",
        "retriever_b = vecstore_b.as_retriever()\n",
        "\n",
        "retrieval = RunnableParallel(\n",
        "    {\n",
        "        \"context_a\": retriever_a,\n",
        "        \"context_b\": retriever_b,\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKtORU5xLpVL"
      },
      "source": [
        "The chain we'll be constructing will look something like this:\n",
        "\n",
        "![](https://github.com/aurelio-labs/langchain-course/blob/main/assets/lcel-flow.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "PNLpfS3_LpVL"
      },
      "outputs": [],
      "source": [
        "chain = retrieval | prompt | llm | output_parser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eo8BfprxLpVL"
      },
      "source": [
        "We `invoke` it as usual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "84gEXUG1LpVL",
        "outputId": "bde3c5f2-a936-40ef-82ab-2aa5091f91fc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'DeepSeek-V3, released in December 2024, uses a Mixture of Experts architecture.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "result = chain.invoke(\n",
        "    \"what architecture does the model DeepSeek released in december use?\"\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcwr16jsLpVL"
      },
      "source": [
        "With that we've seen how we can use `RunnableParallel` and `RunnablePassthrough` to control the flow of data and execution order through our chains.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
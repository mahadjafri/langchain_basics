{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m44oST8ocu18",
        "outputId": "f0225f8d-c516-49f2-e499-af596fd223c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: langchain 0.3.27\n",
            "Uninstalling langchain-0.3.27:\n",
            "  Successfully uninstalled langchain-0.3.27\n",
            "Found existing installation: google-generativeai 0.8.5\n",
            "Uninstalling google-generativeai-0.8.5:\n",
            "  Successfully uninstalled google-generativeai-0.8.5\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.0/444.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.7/241.7 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.5/378.5 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# !pip uninstall -y langchain google-generativeai\n",
        "# !pip install -qU \\\n",
        "#   \"pydantic>=2.11.4,<3\" \\\n",
        "#   \"langchain-core>=0.3.70,<0.4\" \\\n",
        "#   \"langchain-community>=0.3.27,<0.4\" \\\n",
        "#   \"langchain-google-genai>=2.1.10\" \\\n",
        "#   \"google-genai>=0.7.0\" \\\n",
        "#   \"langsmith>=0.3.4\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langchain Setup"
      ],
      "metadata": {
        "id": "cHk6yqOMfTQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from google.colab import userdata\n",
        "\n",
        "# must enter API key\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get(\"LANGCHAIN_API_KEY\") or \\\n",
        "    getpass(\"Enter LangSmith API Key: \")\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"aurelioai-langchain-course-chat-memory-openai-practice\""
      ],
      "metadata": {
        "id": "hr9XFHfHfARy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gemini Setup"
      ],
      "metadata": {
        "id": "Wl1RW0fUfXqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\") or getpass(\n",
        "    \"Enter Gemini API Key: \"\n",
        ")\n",
        "\n",
        "openai_model = \"gemini-2.5-flash-lite\"\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=openai_model,\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    # other params...\n",
        ")"
      ],
      "metadata": {
        "id": "DxUkcfw7fPYw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import (\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        "    ChatPromptTemplate\n",
        ")\n",
        "\n",
        "system_prompt = \"You are a helpful assistant called Kubo.\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
        "])\n",
        "\n",
        "pipeline = (\n",
        "    {\n",
        "        \"query\": lambda x: x[\"query\"],\n",
        "        \"history\": lambda x: x[\"history\"]\n",
        "    }\n",
        "    | prompt_template\n",
        "    | llm)"
      ],
      "metadata": {
        "id": "k3CdyYxifgZT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_chat_map(chat_map_data):\n",
        "    \"\"\"\n",
        "    Print conversation history for each session, including system messages.\n",
        "    Also prints some quick diagnostics if a session looks empty.\n",
        "    \"\"\"\n",
        "    if not chat_map_data:\n",
        "        print(\"Chat map is empty.\")\n",
        "        return\n",
        "\n",
        "    print(\"Sessions:\", list(chat_map_data.keys()))\n",
        "\n",
        "    for session_id, history in chat_map_data.items():\n",
        "        print(f\"\\n{'='*25} Conversation for Session ID: {session_id} {'='*25}\")\n",
        "\n",
        "        # Basic type/len checks\n",
        "        if not hasattr(history, \"messages\"):\n",
        "            print(\"History has no 'messages' attribute.\")\n",
        "            continue\n",
        "        if not isinstance(history.messages, list):\n",
        "            print(f\"'messages' is not a list (got {type(history.messages)}).\")\n",
        "            # Try to render anyway if iterable\n",
        "        if not history.messages:\n",
        "            print(\"No messages in this session.\")\n",
        "            # If your class has internals, print them to help debug:\n",
        "            if hasattr(history, \"summary_text\"):\n",
        "                print(\"summary_text:\", getattr(history, \"summary_text\", None))\n",
        "            if hasattr(history, \"recent\"):\n",
        "                try:\n",
        "                    print(\"recent size:\", len(history.recent))\n",
        "                except Exception:\n",
        "                    pass\n",
        "            continue\n",
        "\n",
        "        # Render messages\n",
        "        for message in history.messages:\n",
        "            role = getattr(message, \"type\", \"unknown\")\n",
        "            content = getattr(message, \"content\", \"\")\n",
        "            if role == \"system\":\n",
        "                print(f\"[SYSTEM] {content}\")\n",
        "            elif role == \"human\":\n",
        "                print(f\"User: {content}\")\n",
        "            elif role == \"ai\":\n",
        "                print(f\"Assistant: {content}\")\n",
        "            else:\n",
        "                print(f\"{role.capitalize()}: {content}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n"
      ],
      "metadata": {
        "id": "xQK0Qmd2rE-w"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation Buffer memory"
      ],
      "metadata": {
        "id": "fk8PifTBjkQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "\n",
        "chat_map = {}\n",
        "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
        "    if session_id not in chat_map:\n",
        "        # if session ID doesn't exist, create a new chat history\n",
        "        chat_map[session_id] = InMemoryChatMessageHistory()\n",
        "    return chat_map[session_id]"
      ],
      "metadata": {
        "id": "xtzT9eVMgpRI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "pipeline_with_history = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\"\n",
        ")"
      ],
      "metadata": {
        "id": "qcvQoDNgg36l"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"Hi, my name is James\"},\n",
        "    config={\"session_id\": \"id_123\"}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aHFrYjNg5JZ",
        "outputId": "8e9bf2a4-9ef6-430b-ceca-6217471b88e6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hi James, it's nice to meet you! How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': []}, id='run--ea9c1ad8-2b83-40a8-bff6-ff08aa06d01c-0', usage_metadata={'input_tokens': 15, 'output_tokens': 18, 'total_tokens': 33, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_map['id_124'].messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPvxcQ_MhQbq",
        "outputId": "e64f0d4b-b95e-4fe0-e614-3fd7a1e554f6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='I am Kubo.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': []}, id='run--e541dd97-a122-4060-ac7b-2ecf5c3cf66f-0', usage_metadata={'input_tokens': 14, 'output_tokens': 4, 'total_tokens': 18, 'input_token_details': {'cache_read': 0}}),\n",
              " HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='I do not have access to your personal information, including your name. I am a large language model, trained by Google.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': []}, id='run--652d6261-ab16-4ff6-930b-5387c8bc7bd8-0', usage_metadata={'input_tokens': 25, 'output_tokens': 25, 'total_tokens': 50, 'input_token_details': {'cache_read': 0}}),\n",
              " HumanMessage(content='Call me Buffalo', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Okay, Buffalo! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': []}, id='run--122aeaf4-cd0b-47d4-9086-0f5e1f1901d3-0', usage_metadata={'input_tokens': 55, 'output_tokens': 11, 'total_tokens': 66, 'input_token_details': {'cache_read': 0}}),\n",
              " HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"You've asked me to call you Buffalo, so I'll refer to you as Buffalo.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': []}, id='run--fd82e365-dd26-431f-9180-f636706fda8d-0', usage_metadata={'input_tokens': 73, 'output_tokens': 20, 'total_tokens': 93, 'input_token_details': {'cache_read': 0}})]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"What is my name?\"},\n",
        "    config={\"session_id\": \"id_124\"}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDO9JANWhn9j",
        "outputId": "2c357528-05bb-4e13-8257-e6f11d2fc760"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"You've asked me to call you Buffalo, so I'll refer to you as Buffalo.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': []}, id='run--fd82e365-dd26-431f-9180-f636706fda8d-0', usage_metadata={'input_tokens': 73, 'output_tokens': 20, 'total_tokens': 93, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation Buffer Window Memory"
      ],
      "metadata": {
        "id": "0G8RkNDrjxIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class BufferWindowMessageHistory(BaseChatMessageHistory, BaseModel):\n",
        "    messages: list[BaseMessage] = Field(default_factory=list)\n",
        "    k: int = 4  # safer default\n",
        "\n",
        "    # Use model_post_init instead of __init__ for side effects\n",
        "    def model_post_init(self, __context):\n",
        "        print(f\"Initialized BufferWindowMessageHistory with k={self.k}\")\n",
        "\n",
        "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
        "        self.messages.extend(messages)\n",
        "        self.messages = self.messages[-self.k:]\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        self.messages = []\n"
      ],
      "metadata": {
        "id": "PNz4oYPxnZkM"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_map: dict[str, BufferWindowMessageHistory] = {}\n",
        "\n",
        "def get_chat_history(session_id: str, k: int = 4) -> BufferWindowMessageHistory:\n",
        "    print(f\"get_chat_history called with session_id={session_id} and k={k}\")\n",
        "    hist = chat_map.get(session_id)\n",
        "    if hist is None:\n",
        "        hist = BufferWindowMessageHistory(k=k)\n",
        "        chat_map[session_id] = hist\n",
        "    # OPTIONAL: if you want k-updates to apply to existing sessions:\n",
        "    # else:\n",
        "    #     hist.k = k\n",
        "    return hist"
      ],
      "metadata": {
        "id": "jQ31nSA1j5Wx"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import ConfigurableFieldSpec\n",
        "\n",
        "pipeline_with_history = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"The session ID to use for the chat history\",\n",
        "            default=\"id_default\",\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"k\",\n",
        "            annotation=int,\n",
        "            name=\"k\",\n",
        "            description=\"The number of messages to keep in the history\",\n",
        "            default=4,\n",
        "        )\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "KPEUOuhNj8qn"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "named_chain = pipeline_with_history.with_config(\n",
        "    run_name=\"Chatbot with Window Memory\"\n",
        ")"
      ],
      "metadata": {
        "id": "NTHO-7Zup9ha"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "named_chain.invoke(\n",
        "    {\"query\": \"My name is Orange\"},\n",
        "    config={\"configurable\": {\"session_id\": \"id_k4\", \"k\": 4}}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9irmznLqbDq",
        "outputId": "bb1de24b-2649-4888-e08d-286df551f2c8"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "get_chat_history called with session_id=id_k4 and k=4\n",
            "Initialized BufferWindowMessageHistory with k=4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hello Orange! It's nice to meet you. How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': []}, id='run--8d0252fd-7a2d-4079-ac9a-c9d9add1fc98-0', usage_metadata={'input_tokens': 13, 'output_tokens': 18, 'total_tokens': 31, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# chat_map[\"id_k4\"].add_user_message(\"I'm researching ice cream flavors.\")\n",
        "# chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what are some examples?\")"
      ],
      "metadata": {
        "id": "OyMJR-b-uyC4"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_chat_map(chat_map)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZFt-aZlqRAZ",
        "outputId": "dd7b48cc-6c22-4ee5-a367-49aa30700a82"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================= Conversation for Session ID: id_k4 =========================\n",
            "Human: My name is Orange\n",
            "Ai: Hello Orange! It's nice to meet you. How can I help you today?\n",
            "Human: I'm researching ice cream flavors.\n",
            "Ai: That's interesting, what are some examples?\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation Summary Memory"
      ],
      "metadata": {
        "id": "mU9FBofDsc9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "from typing import List\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
        "\n",
        "\n",
        "class ConversationSummaryBuffer(BaseChatMessageHistory, BaseModel):\n",
        "    # Public list the wrappers/UI will read\n",
        "    messages: List[BaseMessage] = Field(default_factory=list)\n",
        "\n",
        "    # Internal state\n",
        "    summary_text: str = \"\"\n",
        "    recent_k: int = 4\n",
        "    recent: deque = Field(default_factory=lambda: deque(maxlen=4))\n",
        "\n",
        "    # LLM for summarization\n",
        "    llm: ChatGoogleGenerativeAI = Field(\n",
        "        default_factory=lambda: ChatGoogleGenerativeAI(\n",
        "            model=\"gemini-2.5-flash-lite\", temperature=0\n",
        "        )\n",
        "    )\n",
        "\n",
        "    def model_post_init(self, __context):\n",
        "        # ensure deque has the right maxlen if recent_k was overridden\n",
        "        if self.recent.maxlen != self.recent_k:\n",
        "            self.recent = deque(self.recent, maxlen=self.recent_k)\n",
        "        self._rebuild_messages()  # <-- build initial [System(summary)] + recent\n",
        "\n",
        "    # ---------------- helpers ----------------\n",
        "    def _rebuild_messages(self) -> None:\n",
        "        \"\"\"Recompute public messages list from (summary_text, recent).\"\"\"\n",
        "        sys = SystemMessage(content=self.summary_text or \"(no summary yet)\")\n",
        "        self.messages = [sys] + list(self.recent)\n",
        "\n",
        "    def _render(self, m: BaseMessage) -> str:\n",
        "        role = \"user\" if isinstance(m, HumanMessage) else \\\n",
        "               \"assistant\" if isinstance(m, AIMessage) else \"system\"\n",
        "        return f\"{role}: {m.content}\"\n",
        "\n",
        "    def _fold_into_summary(self, msgs: List[BaseMessage]) -> None:\n",
        "      if not msgs:\n",
        "          return\n",
        "\n",
        "      prompt = ChatPromptTemplate.from_messages([\n",
        "          (\"system\",\n",
        "          \"You are a conversation memory compressor. Maintain a compact summary of OLDER context \"\n",
        "          \"(everything except the most recent verbatim window). The summary must be concise, factual, \"\n",
        "          \"and **paraphrased**. Do NOT copy sentences verbatim. Keep only durable facts, user prefs, \"\n",
        "          \"goals, decisions, constraints, and open questions. Omit chit-chat, fillers, and stylistic text.\"),\n",
        "          (\"system\",\n",
        "          \"HARD LIMITS:\\n\"\n",
        "          \"- Output ≤ {max_chars} characters.\\n\"\n",
        "          \"- No markdown, no lists unless bullet points are necessary.\\n\"\n",
        "          \"- Prefer short clauses separated by ';'.\"),\n",
        "          (\"system\", \"Current summary (may be empty):\\n{summary}\"),\n",
        "          (\"human\",\n",
        "          \"Incorporate these older messages (in order):\\n{older}\\n\\n\"\n",
        "          \"Return ONLY the updated summary, nothing else.\")\n",
        "      ])\n",
        "\n",
        "      older_text = \"\\n\".join(self._render(m) for m in msgs)\n",
        "\n",
        "      updated = (prompt | self.llm).invoke({\n",
        "          \"summary\": self.summary_text,\n",
        "          \"older\":   older_text,\n",
        "          \"max_chars\": 600,          # <-- tune this budget\n",
        "      })\n",
        "\n",
        "      self.summary_text = getattr(updated, \"content\", str(updated))\n",
        "\n",
        "\n",
        "    # ---------------- required API ----------------\n",
        "    def add_messages(self, new_msgs: List[BaseMessage]) -> None:\n",
        "        \"\"\"Append all new messages verbatim; summarize only what falls out.\"\"\"\n",
        "        if not new_msgs:\n",
        "            return\n",
        "\n",
        "        evicted_batch: List[BaseMessage] = []\n",
        "\n",
        "        # Append all at once, collecting anything that will be evicted\n",
        "        for m in new_msgs:\n",
        "            if self.recent_k > 0 and len(self.recent) == self.recent_k:\n",
        "                # peek what will be evicted on append\n",
        "                evicted_batch.append(self.recent[0])\n",
        "            self.recent.append(m)  # deque(maxlen=recent_k) auto-evicts oldest if full\n",
        "\n",
        "        # Fold evicted messages into the running summary in one LLM call\n",
        "        if evicted_batch:\n",
        "            self._fold_into_summary(evicted_batch)\n",
        "\n",
        "        # reflect current state in the public messages list\n",
        "        self._rebuild_messages()  # <-- keep messages in sync\n",
        "\n",
        "    # Convenience\n",
        "    def add_user_message(self, text: str) -> None:\n",
        "        self.add_messages([HumanMessage(content=text)])\n",
        "\n",
        "    def add_ai_message(self, text: str) -> None:\n",
        "        self.add_messages([AIMessage(content=text)])\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        self.summary_text = \"\"\n",
        "        self.recent.clear()\n",
        "        self._rebuild_messages()  # <-- keeps display functions happy"
      ],
      "metadata": {
        "id": "7fttFH1ksjwW"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_map: dict[str, ConversationSummaryBuffer] = {}\n",
        "\n",
        "def get_chat_history(session_id: str, llm: ChatGoogleGenerativeAI, recent_k: int = 4) -> ConversationSummaryBuffer:\n",
        "    print(f\"get_chat_history summary called with session_id={session_id}\")\n",
        "    hist = chat_map.get(session_id)\n",
        "    if hist is None:\n",
        "        hist = ConversationSummaryBuffer(llm=llm, recent_k = recent_k)\n",
        "        chat_map[session_id] = hist\n",
        "    # OPTIONAL: if you want k-updates to apply to existing sessions:\n",
        "    # else:\n",
        "    #     hist.k = k\n",
        "    return hist"
      ],
      "metadata": {
        "id": "OdQuCmmfsrPs"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.runnables import ConfigurableFieldSpec\n",
        "\n",
        "pipeline_with_history = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"The session ID to use for the chat history\",\n",
        "            default=\"id_default\",\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"llm\",\n",
        "            annotation=ChatGoogleGenerativeAI,\n",
        "            name=\"LLM\",\n",
        "            description=\"The LLM to use for the conversation summary\",\n",
        "            default=llm,\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"recent_k\",\n",
        "            annotation=int,\n",
        "            name=\"Recent K\",\n",
        "            description=\"The number of recent messages to save as is\",\n",
        "            default=4,\n",
        "        )\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "efUo2CRzstb2"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "named_chain = pipeline_with_history.with_config(\n",
        "    run_name=\"Chatbot with Summary memory and configurable K\"\n",
        ")"
      ],
      "metadata": {
        "id": "jEdtQiTYsv4g"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "named_chain.invoke(\n",
        "    {\"query\": \"What are my interests\"},\n",
        "    config={\"configurable\": {\"session_id\": \"id_sum_k_1\", \"llm\": llm, \"recent_k\": 4}}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WYqv88qs_v_",
        "outputId": "9cc000ff-a43d-4824-a760-bda1efcb1719"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "get_chat_history summary called with session_id=id_sum_k_1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hobb, based on our previous conversations, your interests include:\\n\\n*   **Training bears to eat with forks.**\\n*   **Fighting ducks.**\\n*   You previously declined help.\\n*   You asked if bears like ice cream, and I advised against feeding it to them due to health concerns.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': []}, id='run--22f2c8d0-c803-41d0-bbe3-12ef90cd7f94-0', usage_metadata={'input_tokens': 97, 'output_tokens': 63, 'total_tokens': 160, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_map[\"id_sum_k_1\"].add_user_message(\"Any similar flavors?\")\n",
        "# chat_map[\"id_sum_k_1\"].add_ai_message(\"Almond is similar\")"
      ],
      "metadata": {
        "id": "lJhx66srURkK"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_chat_map(chat_map)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_S33TCZKs0lC",
        "outputId": "8472ca34-9801-44dd-8755-d1bf5b894124"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sessions: ['id_sum_k_1']\n",
            "\n",
            "========================= Conversation for Session ID: id_sum_k_1 =========================\n",
            "[SYSTEM] User's name is Hobb; user trains bears to eat with forks; user likes to fight ducks; user previously declined help; user asked if bears like ice cream; assistant advised against feeding ice cream to bears due to health concerns; user asked for \"hello\" in French, assistant provided \"Bonjour\".\n",
            "User: What is hello in spanish\n",
            "Assistant: \"Hello\" in Spanish is **\"Hola\"**.\n",
            "User: What are my interests\n",
            "Assistant: Hobb, based on our previous conversations, your interests include:\n",
            "\n",
            "*   **Training bears to eat with forks.**\n",
            "*   **Fighting ducks.**\n",
            "*   You previously declined help.\n",
            "*   You asked if bears like ice cream, and I advised against feeding it to them due to health concerns.\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}